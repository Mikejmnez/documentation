
<!DOCTYPE html>
<html lang="en" dir="ltr" class="client-nojs">
<head>
<title>Satellite Swath Data Aggregation - OPeNDAP Documentation</title>
</head>
<body class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-Satellite_Swath_Data_Aggregation skin-monobook action-view">
<div id="globalWrapper">
<div id="column-content"><div id="content" class="mw-body-primary" role="main">
	
	
	<h1 id="firstHeading" class="firstHeading" lang="en"><span dir="auto">Satellite Swath Data Aggregation</span></h1>
	<div id="bodyContent" class="mw-body">
		<div id="siteSub">From OPeNDAP Documentation</div>
		<div id="contentSub"></div>
		

		<!-- start content -->
<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><p><a href="../index.php/Use_cases_for_swath_and_time_series_aggregation" title="Use cases for swath and time series aggregation"> &lt;-- Back</a>
</p><p>'<b>Point Of Contact:</b> <i>James</i>
</p>
<div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Description"><span class="tocnumber">1</span> <span class="toctext">Description</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Goal"><span class="tocnumber">2</span> <span class="toctext">Goal</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Summary"><span class="tocnumber">3</span> <span class="toctext">Summary</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#Actors"><span class="tocnumber">4</span> <span class="toctext">Actors</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#Preconditions"><span class="tocnumber">5</span> <span class="toctext">Preconditions</span></a></li>
<li class="toclevel-1 tocsection-6"><a href="#Triggers"><span class="tocnumber">6</span> <span class="toctext">Triggers</span></a></li>
<li class="toclevel-1 tocsection-7"><a href="#Basic_Flow"><span class="tocnumber">7</span> <span class="toctext">Basic Flow</span></a>
<ul>
<li class="toclevel-2 tocsection-8"><a href="#Aside:_The_granule_list"><span class="tocnumber">7.1</span> <span class="toctext">Aside: The granule list</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Aside:_data_organization"><span class="tocnumber">7.2</span> <span class="toctext">Aside: data organization</span></a>
<ul>
<li class="toclevel-3 tocsection-10"><a href="#Regarding_time"><span class="tocnumber">7.2.1</span> <span class="toctext">Regarding time</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-11"><a href="#Alternate_Flow"><span class="tocnumber">8</span> <span class="toctext">Alternate Flow</span></a></li>
<li class="toclevel-1 tocsection-12"><a href="#Post_Conditions"><span class="tocnumber">9</span> <span class="toctext">Post Conditions</span></a></li>
<li class="toclevel-1 tocsection-13"><a href="#Activity_Diagram"><span class="tocnumber">10</span> <span class="toctext">Activity Diagram</span></a></li>
<li class="toclevel-1 tocsection-14"><a href="#Notes"><span class="tocnumber">11</span> <span class="toctext">Notes</span></a>
<ul>
<li class="toclevel-2 tocsection-15"><a href="#Information_about_the_data"><span class="tocnumber">11.1</span> <span class="toctext">Information about the data</span></a></li>
<li class="toclevel-2 tocsection-16"><a href="#Technical_approaches_to_the_problem"><span class="tocnumber">11.2</span> <span class="toctext">Technical approaches to the problem</span></a>
<ul>
<li class="toclevel-3 tocsection-17"><a href="#Using_the_BES_define_command"><span class="tocnumber">11.2.1</span> <span class="toctext">Using the BES define command</span></a></li>
<li class="toclevel-3 tocsection-18"><a href="#Using_a_server_function"><span class="tocnumber">11.2.2</span> <span class="toctext">Using a server function</span></a>
<ul>
<li class="toclevel-4 tocsection-19"><a href="#The_server_function_interface"><span class="tocnumber">11.2.2.1</span> <span class="toctext">The server function interface</span></a>
<ul>
<li class="toclevel-5 tocsection-20"><a href="#Specification_of_the_granules"><span class="tocnumber">11.2.2.1.1</span> <span class="toctext">Specification of the granules</span></a></li>
<li class="toclevel-5 tocsection-21"><a href="#Variables_to_include_in_the_response"><span class="tocnumber">11.2.2.1.2</span> <span class="toctext">Variables to include in the response</span></a></li>
<li class="toclevel-5 tocsection-22"><a href="#Space_and_Time_constraints"><span class="tocnumber">11.2.2.1.3</span> <span class="toctext">Space and Time constraints</span></a></li>
</ul>
</li>
<li class="toclevel-4 tocsection-23"><a href="#Response_Structure"><span class="tocnumber">11.2.2.2</span> <span class="toctext">Response Structure</span></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-24"><a href="#Resources"><span class="tocnumber">12</span> <span class="toctext">Resources</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Description"><span class="mw-headline-number">1</span> Description</span></h2>
<p>Satellite_Swath_1
</p>
<h2><span class="mw-headline" id="Goal"><span class="mw-headline-number">2</span> Goal</span></h2>
<p>The user wants to get data from several granules of a many-granule data set using a single request. The user does not want to iterate over several response objects/files; all the data should be contained in a single entity. However, this aggregation operation must work with Swath data from a satellite, which poses special problems. Furthermore, the aggregation should require little or no effort by the data provider to configure.
</p>
<h2><span class="mw-headline" id="Summary"><span class="mw-headline-number">3</span> Summary</span></h2>
<p>Principal actor: A person who wants to access level 2 satellite swath data from a number of granules using a single URL. The person may be an end user who issues a data request or they may be the developer of a system that will build a URL to return to an end user. In either case, an end-user dereferences the URL to get data.
</p><p>Goal: Eliminate users having to use many URLs to get these data.
</p><p>The data are Satellite Swath data (e.g., MODIS, level2) where each granule contains a number of dependent variables are matched to lat and lon arrays, all of which are 2D (this is the general case for a 2D discrete coverage). The data set is made up of a number of these granules, organized in a hierarchy of directories by date. Each of the granules holds one 'pass' of the satellite from one pole to another. When a user searches for data, they use a spatial and time box to choose values from the whole data set. Currently the response to that search is a list of granules, where each contains <i>some</i> of the data in the users query. The complete granules, or subset versions, may be downloaded. In the either case, the user must access/download a number of files and then the values combined somehow - the exact mechanism being left up to the end user, including reading the files that contain the data.
</p><p>In this use case, the user instead uses a single URL to access all of the data that match the selection criteria. Those values will be returned using the CSV format, so there is virtually no file decoding burden put on the user. The URL will be one that a system developer can easily build in software as well as one that a person could write by hand, at least in many cases.
</p>
<h2><span class="mw-headline" id="Actors"><span class="mw-headline-number">4</span> Actors</span></h2>
<dl>
<dt> A developer building a 'response URL' from a user's data search request
</dt>
<dd> This person must write software that will translate the search request into the Hyrax server's (to-be developed) aggregation request syntax. 
</dd>
<dt> A person making a data request to the Hyrax server
</dt>
<dd> This actor will need to a similar understanding of the new aggregation mechanism as the 'developer' but will be writing the URLs 'by hand'.
</dd>
<dt> The Hyrax data server
</dt>
<dd> This is an important actor because the server will have to be updated to realize the new software's benefits.
</dd>
<dt> Level 2 satellite swath data, stored in multiple files (file == granule == 'pass')
</dt>
<dd> This use case is limited to a particular kind of data, although the resulting software might be useful for Time Series data too.
</dd>
</dl>
<h2><span class="mw-headline" id="Preconditions"><span class="mw-headline-number">5</span> Preconditions</span></h2>
<dl>
<dt>The data to be accessed are served using Hyrax
</dt>
<dd>Yup
</dd>
<dt>We may require that the <i>CF</i> option of the HDF4 handler be turned on
</dt>
<dd>This will make the data values much easier to use because access will be as if we're working with a netCDF3 file - no hierarchy to translate, no weird Sequence of Structure variables that correlate to dependent var dimensions.
</dd>
<dt>The Hyrax server has been updated to include the aggregation function
</dt>
<dd>Ditto, but this will require that system admins install the code.
</dd>
<dt>The user's software understands the structure of the returned data values
</dt>
<dd>Again, Yup. In this use case that means the data are in returned in a CSV table, encoded using ASCII characters. Excel can read these kinds of responses, e.g., but it's kind of an unusual way to package satellite data.
</dd>
<dt>The data are indexed by a search system
</dt>
<dd>The main use-case - the scenario where a search system would normally return a list of URLs instead returns a single URL that will call the aggregation function that will return all of the granules' data in one shot.
</dd>
<dt>The user invoking the aggregation function understands the organization of the data
</dt>
<dd>This is actually a variant of the main use-case because the in the variant scenario the user builds the URL that calls the aggregation function and not a search system
</dd>
</dl>
<h2><span class="mw-headline" id="Triggers"><span class="mw-headline-number">6</span> Triggers</span></h2>
<ul>
<li> A user searches for data and the result indicates that the data they want are spread over a number of granules.
</li>
<li> A user knows they want data that are (or may be) spread over a number of granules in a dataset.
</li>
</ul>
<h2><span class="mw-headline" id="Basic_Flow"><span class="mw-headline-number">7</span> Basic Flow</span></h2>
<p>A user (the actor that initiates the use case) performs a search using EDSC and the result set contains two or more granules. The search client would normally return a list of URLs to the discrete granules that make up the result set. However, in this use case, the client has been programed to recognize this situation and will respond by forming a URL that will run the aggregation server-side operation and request the aggregated data be returned as a list of CSV data points.
</p><p>The server's internal software will build the response's table encoded as a DAP Sequence; the server will transform that into CSV if that is part of the request URL.
</p><p>To formulate a request, the client will need to provide three pieces of information:
</p>
<ul>
<li> The granules to consider when building the aggregation - <font color="red">explicitly enumerated</font>;
</li>
<li> The (dependent) variables within those granules to include in the aggregation;
</li>
<li> The space and time bounds (the <i>independent</i> variables) that will be used to constraint values of the <i>dependent</i> variables; and 
</li>
<li> <font color="red">Assumption: The independent and dependent variables are all present in the granules</font>.<del>where to find those independent variables (in the granules' variables, attributes or filenames - we should probably limit the first iteration to just variables if we can, see below).</del>
</li>
</ul>
<p>These parameters will be passed to the server using some sort of a constraint. <del>If the granules to aggregate can be specified using a regex, then it should be possible to use HTTP's GET verb.</del> <font color="red">Since each granule must be listed separately, POST will be used to make the request</font>. Because we know that the aggregation operation will be be working with Level 2 satellite swath (geospatial and temporal) data, some optimizations can be made. We know that latitude, longitude and time are encoded in these granules for each sample point. Thus the cases where time is encoded in an attribute or the granule name don't need to be addressed. (They might be addressed by a future version of the code, however.)
</p><p>Because of variations in time representation, we may adopt ISO8601 as the only way to specify time.
</p><p><font color="red">The web service end point used to access the aggregation will take a Request Document using HTTP POST. The request will contain the list of granules in its body, one granule per line. The remaining parameters will be passed in using the HTTP Query String (i.e., as keyname-value pairs).
</p>
<dl>
<dt>Using this web service will look something like
</dt>
<dd>http://host/server/aggregator?d4_func=table(vars)&amp;d4_ce=constraints&amp;return_as=csv
</dd>
</dl>
<p>where <i>vars</i> and <i>sub-expressions</i> are:
</p>
<dl>
<dt>vars
</dt>
<dd>A list of variables as they are named in the granules (e.g., Cloud_Mask_QA, Mass_Concentration_Land)
</dd>
<dt>constraints
</dt>
<dd>A list of strings that denote the bounds of values that limit the request in space and time. (e.g., "table|-120 &lt;= Longitude &lt; 20" where <i>Longitude</i> is an independent variable in the dataset)</font>.
</dd>
</dl>
<p>The response from the request will be a CSV table listing the dependent variables followed by the values of the dependent vars. For example, the response would typically look like:
</p>
<pre>
Latitude, Longitude, Scan_Start_Time, Cloud_Mask_QA, Mass_Concentration_Land
45, -120, 2010/10/01, 14, 127
.
.
.
</pre>
<p>where each granule would have 203 * 135 (25,375) rows (one for each cell along and across the swath) maximum; fewer given space/time constraints.
</p><p>The CSV tabular response is received as a text/plain type HTTP/HTML response by the client software.
</p><p><font color="red">
</p>
<h4><span class="mw-headline" id="Aside:_The_granule_list"><span class="mw-headline-number">7.1</span> Aside: The granule list</span></h4>
<p>Because the search tool is used to build the list of granules and it performs 'point-in-box' (or point-in-polygon, which subsumes PIB) selection of granules, this web service will assume that every granule in the request document contains <i>some</i> data that should be included in the response</font>.
</p>
<h4><span class="mw-headline" id="Aside:_data_organization"><span class="mw-headline-number">7.2</span> Aside: data organization</span></h4>
<p>NB: This is discussed in more detail below in the Notes section.
</p><p>The Independent and dependent vars are all (with one important caveat) two-dimensional arrays. The mapping between a Lat/lon/time tuple and a dependent variable's value is made by using the same (i,j) indices to access both the independent and dependent variables.
</p><p>For example, a granule might contain these arrays:
independent vars: Lat[5][7]; Lon[5][7]; time[5][7]
dependent vars: SST[5][7]; Wind_Speed[5][7]
</p><p>To get the wind speed at a given time, lat and lon, find those values in the independent vars, note the indices and then access the Wind_Speed array at those index values.
</p><p>The <b>caveat</b> is that most of the dependent vars in a L2 MODIS granule have three dimensions where the additional dim is some other independent variable. There are two ways we can accommodate that, but both require some moderately detailed knowledge on the part of the user:
</p>
<ul>
<li> We can accommodate dependent vars with an extra dimension like [MODIS_band][][] by using something like "MODIS_Band == 440"
</li>
<li> We can allow a dependent var to be specified using a partial dimension list and use that as a subset. For example, given <i>Mean_Reflectance_Land[MODIS_Band_Land = 7][Cell_Along_Swath = 203][Cell_Across_Swath = 135]</i> where <i>Cell_Along_Swath</i> and <i>Cell_Across_Swath</i> match the indices of Latitude, Longitude and ...time..., the dependent variable can be given using <i>Mean_Reflectance_Land[0]</i> or in general <i>Mean_Reflectance_Land[<b>projection expression</b>]</i>
</li>
</ul>
<p>To make this work, the value(s) associated with the 'extra dimension' will be turned into columns in the response table.
</p>
<h5><span class="mw-headline" id="Regarding_time"><span class="mw-headline-number">7.2.1</span> Regarding time</span></h5>
<p>Level 2 data contain samples at various lat/lon points made over time. Each granule has a start time and end time, as does any temporally-contiguous set of granules. If we think about a request for all data that fall within two points on a time line, then the set of potential files with data can be thought of as having three kinds of elements: The file that contains the starting time, along with zero or more previous times; the file that contains the ending time, along with zero or more later times; and the 'interior' files where all of the data are within the selection bounds.
</p>
<h2><span class="mw-headline" id="Alternate_Flow"><span class="mw-headline-number">8</span> Alternate Flow</span></h2>
<p>There are a number of alternate flows involving errors, all of which involve invalid parameters or granules that fail in some way.
</p><p>A significant alternate flow is that a user can build the URL that makes the request themselves. Nothing about the request or the response changes, however. The significant difference is that a computer program does not have to figure out how to make the URL. In the main flow of this use-case, it does.
</p>
<h2><span class="mw-headline" id="Post_Conditions"><span class="mw-headline-number">9</span> Post Conditions</span></h2>
<p>The client will have the data values, in CSV-encoded tabular form.
</p>
<h2><span class="mw-headline" id="Activity_Diagram"><span class="mw-headline-number">10</span> Activity Diagram</span></h2>
<p>[skipped]
</p>
<h2><span class="mw-headline" id="Notes"><span class="mw-headline-number">11</span> Notes</span></h2>
<p>Where to find sample data for this use case:
MODAPS (MODIS):
</p>
<ul>
<li> FTP server is here: <a rel="nofollow" class="external free" href="ftp://nrt1.modaps.eosdis.nasa.gov/allData/1/">ftp://nrt1.modaps.eosdis.nasa.gov/allData/1/</a>
</li>
<li> Log in with your URS credentials (create an account here: <a rel="nofollow" class="external free" href="https://urs.earthdata.nasa.gov">https://urs.earthdata.nasa.gov</a>)
</li>
<li> Folders ending in _L2 contain level 2 data.
</li>
</ul>
<h3><span class="mw-headline" id="Information_about_the_data"><span class="mw-headline-number">11.1</span> Information about the data</span></h3>
<p><b>NB:</b> This is how the files look when the CF option is turned on for the HDF4 handler. They are more messy without it.
</p><p>There are 83 variables in the HDF4 files. I downloaded three files, all were identical in the variables they held - same names and types. I determined this by looking at the DDS responses for the variables.
</p><p>In the files, there are three independent variables that are the same 'shape':
</p>
<ul>
<li> Float64 Scan_Start_Time[Cell_Along_Swath = 203][Cell_Across_Swath = 135]
</li>
<li> Float32 Longitude[Cell_Along_Swath = 203][Cell_Across_Swath = 135]
</li>
<li> Float32 Latitude[Cell_Along_Swath = 203][Cell_Across_Swath = 135]
</li>
</ul>
<p>Of the 83 variables, 71 (including the three above) have the dimensions <i>[Cell_Along_Swath = 203][Cell_Across_Swath = 135]</i>.
</p><p>Of those 71, 27 (including Lat, ...) have only the dimensions <i>[Cell_Along_Swath = 203][Cell_Across_Swath = 135]</i>. They are:
</p>
<pre>
    Int16 Aerosol_Type_Land[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Angstrom_Exponent_Land[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Cloud_Fraction_Land[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Cloud_Fraction_Ocean[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int32 Cloud_Mask_QA[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Corrected_Optical_Depth_Land_wav2p1[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Deep_Blue_Aerosol_Optical_Depth_550_Land[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Deep_Blue_Aerosol_Optical_Depth_550_Land_STD[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Deep_Blue_Angstrom_Exponent_Land[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Fitting_Error_Land[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Image_Optical_Depth_Land_And_Ocean[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Float32 Latitude[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Float32 Longitude[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Float32 Mass_Concentration_Land[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Number_Pixels_Used_Ocean[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Optical_Depth_Land_And_Ocean[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Optical_Depth_Ratio_Small_Land[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Optical_Depth_Ratio_Small_Land_And_Ocean[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int32 Quality_Assurance_Crit_Ref_Land[Cell_Along_Swath = 203][Cell_Across_Swath = 135][QA_Byte_Land = 5];
    Int32 Quality_Assurance_Land[Cell_Along_Swath = 203][Cell_Across_Swath = 135][QA_Byte_Land = 5];
    Int32 Quality_Assurance_Ocean[Cell_Along_Swath = 203][Cell_Across_Swath = 135][QA_Byte_Ocean = 5];
    Float64 Scan_Start_Time[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Scattering_Angle[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Sensor_Azimuth[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Sensor_Zenith[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Solar_Azimuth[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Solar_Zenith[Cell_Along_Swath = 203][Cell_Across_Swath = 135];
</pre>
<p>The other ones, which have other dimensions, are (sorted and grouped by the additional dimension, which always comes first):
</p>
<pre>
    Int16 Mean_Reflectance_Land[MODIS_Band_Land = 7][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 STD_Reflectance_Land[MODIS_Band_Land = 7][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Asymmetry_Factor_Average_Ocean[MODIS_Band_Ocean = 7][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Asymmetry_Factor_Best_Ocean[MODIS_Band_Ocean = 7][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Backscattering_Ratio_Average_Ocean[MODIS_Band_Ocean = 7][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Backscattering_Ratio_Best_Ocean[MODIS_Band_Ocean = 7][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Effective_Optical_Depth_Average_Ocean[MODIS_Band_Ocean = 7][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Effective_Optical_Depth_Best_Ocean[MODIS_Band_Ocean = 7][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Mean_Reflectance_Ocean[MODIS_Band_Ocean = 7][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Optical_Depth_Large_Average_Ocean[MODIS_Band_Ocean = 7][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Optical_Depth_Large_Best_Ocean[MODIS_Band_Ocean = 7][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Optical_Depth_Small_Average_Ocean[MODIS_Band_Ocean = 7][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Optical_Depth_Small_Best_Ocean[MODIS_Band_Ocean = 7][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 STD_Reflectance_Ocean[MODIS_Band_Ocean = 7][Cell_Along_Swath = 203][Cell_Across_Swath = 135];

    Int16 Aerosol_Cldmask_Byproducts_Land[Num_By_Products = 7][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Aerosol_Cldmask_Byproducts_Ocean[Num_By_Products = 7][Cell_Along_Swath = 203][Cell_Across_Swath = 135];

    Int16 Deep_Blue_Aerosol_Optical_Depth_Land[Num_DeepBlue_Wavelengths = 3][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Deep_Blue_Aerosol_Optical_Depth_Land_STD[Num_DeepBlue_Wavelengths = 3][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Deep_Blue_Mean_Reflectance_Land[Num_DeepBlue_Wavelengths = 3][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Deep_Blue_Number_Pixels_Used_Land[Num_DeepBlue_Wavelengths = 3][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Deep_Blue_Single_Scattering_Albedo_Land[Num_DeepBlue_Wavelengths = 3][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Deep_Blue_Surface_Reflectance_Land[Num_DeepBlue_Wavelengths = 3][Cell_Along_Swath = 203][Cell_Across_Swath = 135];

    Int16 Critical_Reflectance_Land[Solution_1_Land = 2][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Error_Critical_Reflectance_Land[Solution_1_Land = 2][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Error_Path_Radiance_Land[Solution_1_Land = 2][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Number_Pixels_Used_Land[Solution_1_Land = 2][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Path_Radiance_Land[Solution_1_Land = 2][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 QualityWeight_Critical_Reflectance_Land[Solution_1_Land = 2][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 QualityWeight_Path_Radiance_Land[Solution_1_Land = 2][Cell_Along_Swath = 203][Cell_Across_Swath = 135];

    Int16 Surface_Reflectance_Land[Solution_2_Land = 3][Cell_Along_Swath = 203][Cell_Across_Swath = 135];

    Int16 Corrected_Optical_Depth_Land[Solution_3_Land = 3][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Mean_Reflectance_Land_All[Solution_3_Land = 3][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Standard_Deviation_Reflectance_Land_All[Solution_3_Land = 3][Cell_Along_Swath = 203][Cell_Across_Swath = 135];

    Int16 Optical_Depth_Small_Land[Solution_4_Land = 4][Cell_Along_Swath = 203][Cell_Across_Swath = 135];

    Int16 Optical_Depth_by_models_ocean[Solution_Index = 9][Cell_Along_Swath = 203][Cell_Across_Swath = 135];

    Float32 Cloud_Condensation_Nuclei_Ocean[Solution_Ocean = 2][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Float32 Mass_Concentration_Ocean[Solution_Ocean = 2][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Angstrom_Exponent_1_Ocean[Solution_Ocean = 2][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Angstrom_Exponent_2_Ocean[Solution_Ocean = 2][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Effective_Radius_Ocean[Solution_Ocean = 2][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Least_Squares_Error_Ocean[Solution_Ocean = 2][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Optical_Depth_Ratio_Small_Ocean_0_55micron[Solution_Ocean = 2][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Solution_Index_Ocean_Large[Solution_Ocean = 2][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
    Int16 Solution_Index_Ocean_Small[Solution_Ocean = 2][Cell_Along_Swath = 203][Cell_Across_Swath = 135];
</pre>
<p>Here are those 'additional dimensions' along with their sizes:
</p>
<ul>
<li> MODIS_Band_Land = 7
</li>
<li> Num_By_Products = 7
</li>
<li> Num_DeepBlue_Wavelengths = 3
</li>
<li> Solution_1_Land = 2
</li>
<li> Solution_2_Land = 3
</li>
<li> Solution_3_Land = 3
</li>
<li> Solution_4_Land = 4
</li>
<li> Solution_Index = 9
</li>
<li> Solution_Ocean = 2
</li>
</ul>
<p>Here are the values of those dimensions - these are like DAP2 Grid Maps:
</p>
<pre>
Dataset: MOD04_L2.A2015021.0020.051.NRT.hdf
MODIS_Band_Land, 470, 555, 659, 865, 1240, 1640, 2130
MODIS_Band_Ocean, 470, 555, 659, 865, 1240, 1640, 2130
Solution_1_Land, 470, 659
Solution_2_Land, 470, 555, 659
Solution_3_Land, 470, 659, 2130
Solution_Ocean, 1, 2
Solution_Index, 1, 2, 3, 4, 5, 6, 7, 8, 9
Num_DeepBlue_Wavelengths, 0, 0, 0
</pre>
<h3><span class="mw-headline" id="Technical_approaches_to_the_problem"><span class="mw-headline-number">11.2</span> Technical approaches to the problem</span></h3>
<p><b>NB:</b> Unresolved issues in bold below.
</p><p>There are two general technical solutions to this problem, both requiring iteration over a number of granules. We can use the BES's <a href="../index.php/BES_XML_Commands#define" title="BES XML Commands"> <i>define</i></a> command to do this or the capability can be built into the aggregating code. 
</p><p>For a basic request, for each granule: 
</p>
<ul>
<li> Read the Scan_Start_Time, Longitude and Latitude values (3 time 25,375 values; on array of doubles and two of floats; 25375 * 8 + 2(25375 * 4) = 406,000 bytes)
</li>
<li> Read one of more of the arrays of values. Again 25,375 elements, mostly Int16 arrays. It's probably not worth subsetting the read. The memory required for several dependent variables is going to be small (~ 0.5 MB for 4 values; so four dependent vars and the time/lat/lon values can all fit in ~ 1MB).
</li>
<li> For an aggregation that spans N granules in time, the result will include all of the time of N-2 (When N&gt;2) of the granules. The aggregator only needs to examine the time values for the 'end points'.
</li>
<li> <b>Finding the endpoints in the collection of files will be important. How do we generalize this? Time information is nominally in the file names, but...</b> 
</li>
<li> Since most of the dependent vars have an extra dimension that is used to select a band (wavelength) or other facet, that will have to be accommoated. Two ideas are presented int he <a href="#Basic_Flow"> <i>Basic Flow</i></a> section.
</li>
<li> If the client requests several dependent vars they have to agree in all their dimensions, both number and 'kind'. This requirement keeps the result limited to values that can be represented in a single table.
</li>
</ul>
<h4><span class="mw-headline" id="Using_the_BES_define_command"><span class="mw-headline-number">11.2.1</span> Using the BES define command</span></h4>
<p>The BES  <a href="../index.php/BES_XML_Commands#define" title="BES XML Commands"> <i>define</i></a> command can aggregate a set of requests into a DAP2 Structure. We could modify/extend this to include Sequences, or process a Structure or Array of Sequences transforming that into a single Sequence. The challenges with using the BES aggregation capability is that it is not obvious how that would be used to build the DAP metadata responses. I think the <i>return_as</i> capability of the BES could be used to transform the data in CSV encoding.
</p>
<h4><span class="mw-headline" id="Using_a_server_function"><span class="mw-headline-number">11.2.2</span> Using a server function</span></h4>
<p>The aggregation operation could be implemented using a server function, but that function would either need to make use of the BES's dispatch and iteration capabilities, something that the BES design never anticipated, or provide its own, something this is potentially quite complex. Below is a rough specification for such a function, without addressing the issues of dispatch and iteration over the granules. The design captures some useful information even if it is not actually used.
</p>
<h5><span class="mw-headline" id="The_server_function_interface"><span class="mw-headline-number">11.2.2.1</span> The server function interface</span></h5>
<p>The aggregation server function needs to know what granules to aggregation on, the variables that are to be returned (nominally the returned variables are a subset of all the variables in the granules) and the space-time constraints that data must satisfy. The return format (DAP2 or DAP4 binary; CSV; or NetCDF file is determined using the request extension of the data access URL.
</p>
<h6><span class="mw-headline" id="Specification_of_the_granules"><span class="mw-headline-number">11.2.2.1.1</span> Specification of the granules</span></h6>
<p>The specification of granules will use a regular expression. This will provide a way for callers of the function to limit the granules using various information encoded in the filenames as well as specifying all of the files in (or under) a given location in the server's file system. For example, a user might want only ascending passes or only passes made during daylight. Often L2 data files encode this kind of information.
</p><p>One issue with this is that there's no standard way to make the more fine-grained distinctions (e.g., passes that are on the ascending part of the satellite's orbit), so how a user or search client would know apply this algorithmically is hard to say.
</p>
<h6><span class="mw-headline" id="Variables_to_include_in_the_response"><span class="mw-headline-number">11.2.2.1.2</span> Variables to include in the response</span></h6>
<p>The caller will list a number of names. The function will assume that every granule that matches the regex will contain all of the variables. Each variable is assumed to hold 'dependent values'. For any given granule (maybe all granules?) the listed variables may not have any values included in the response because no values may have sampled with the space-time constraint.
</p>
<h6><span class="mw-headline" id="Space_and_Time_constraints"><span class="mw-headline-number">11.2.2.1.3</span> Space and Time constraints</span></h6>
<p>Two pieces of information will be provided to specify the space time constraint. The list of variables that contain the latitude, longitude and time values will be given along with the constraints on their values. To make it easier to unambiguously associated the variable with the constraint, the limitations will be made using 'mini expressions' of the form <i>value relop var</i> or <i>value relop var relop value</i>. If one <i>var</i> appears in more than one of these expressions the result will be the intersection of the values specified by the expressions. 
</p><p>The parameter specification is designed to be flexible enough to specify the constraints without having to configure the function for each dataset. The downside is that it will not take into account the specifics of latitude, longitude or time. For example, geospatial subsetting often takes into account that longitude values 'wrap' at either the dateline or prime meridian. The scheme used here will not do that, which means it can be applied to <i>any</i> independent variables' values. For these data (level 2) that will not be a problem because the values are returned in a table.
</p>
<h5><span class="mw-headline" id="Response_Structure"><span class="mw-headline-number">11.2.2.2</span> Response Structure</span></h5>
<p>The response will be in the form of a table of values. The table will have columns that list the independent variables first and then the dependent variables. Only rows with all values will be included.
</p>
<h2><span class="mw-headline" id="Resources"><span class="mw-headline-number">12</span> Resources</span></h2>
<table cellspacing="0" border="1">
<tr>
<td>Resource
</td>
<td>Owner
</td>
<td>Description
</td>
<td>Availability
</td>
<td>Source System
</td></tr>
<tr>
<td>Hyrax server
</td>
<td>Data center (e.g., NSIDC, JPL)
</td>
<td>Data server configured both for the file type and with the new extensions that enable aggregation
</td>
<td>This should be available all the time
</td>
<td>?
</td></tr>
<tr>
<td>Web server/servlet engine
</td>
<td>Data center
</td>
<td>The data center must run the supporting web infrastructure
</td>
<td>All the time
</td>
<td>?
</td></tr>
<tr>
<td>Data provider
</td>
<td>Data center
</td>
<td>A person who understands the data and can answer questions about its contents
</td>
<td>Business hours
</td>
<td>?
</td></tr></table>




</div>
				<!-- end content -->
				<div class="visualClear"></div>
	</div>
</div></div>
</body></html>